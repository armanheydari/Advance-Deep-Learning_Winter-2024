{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armanheydari/Advance-Deep-Learning_Winter-2024/blob/main/Assignment2/cmpt489_828_a2_q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CMPT 489/828 Assignment 2**\n",
        "\n",
        "Follow the instructions in this notebook and complete the missing code.\n",
        "\n",
        "**NOTE: Do Not Change Any Provided Code or Given Variable Names!**"
      ],
      "metadata": {
        "id": "HGh9xVJYL_e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1**. Create a simple fully-connected neural network from scratch (**30 points**)\n",
        "\n",
        "**NOTE: Do Not Use torch.nn Module for This Question! (Except for nn.Softmax())**"
      ],
      "metadata": {
        "id": "iue8Vhhv9zny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "nRihqwkFZmzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load CIFAR-10 dataset with pytorch\n",
        "# convert to tensor, normalize and flatten\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    transforms.Lambda(lambda x: torch.flatten(x)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_id = list(range(4000))\n",
        "val_id = list(range(4000, 5000))\n",
        "test_id = list(range(500))\n",
        "\n",
        "# subset dataset and create dataloader with batch_size=1\n",
        "train_sub_set = torch.utils.data.Subset(trainset, train_id)\n",
        "val_sub_set = torch.utils.data.Subset(trainset, val_id)\n",
        "test_sub_set = torch.utils.data.Subset(testset, test_id)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_sub_set, batch_size=1, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_sub_set, batch_size=1, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_sub_set, batch_size=1, shuffle=True)\n",
        "\n",
        "# check data size, should be CxHxW, class map only useful for visualization and sanity checks\n",
        "image_size = trainset[0][0].size(0)\n",
        "class_map = {0: 'plane', 1: 'car', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship',\n",
        "             9: 'truck'}"
      ],
      "metadata": {
        "id": "FzV2t1meZ3Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Implement a fully-connected model (**16 points**)"
      ],
      "metadata": {
        "id": "_c4d51xzk8vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement operations for our model\n",
        "\n",
        "def activation(x):\n",
        "    \"\"\"\n",
        "    Implement activation function with tanh()\n",
        "    :param x: input tensor\n",
        "    :return: output tensor equals element-wise tanh(x)\n",
        "    \"\"\"\n",
        "    ###############################################################################\n",
        "    # TODO:                                                                       #\n",
        "    # 1. calculate act = tanh(x)                                                  #\n",
        "    ###############################################################################\n",
        "    # *****BEGIN YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    act =\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return act\n",
        "\n",
        "\n",
        "def activation_grad(x):\n",
        "    \"\"\"\n",
        "    Calculate the gradient of activation() respect to input x\n",
        "    You need to find the maths representation of the derivative first\n",
        "    :param x: input tensor\n",
        "    :return: element-wise gradient of activation()\n",
        "    \"\"\"\n",
        "    ###############################################################################\n",
        "    # TODO:                                                                       #\n",
        "    # 1. find maths represenation of activation()                                 #\n",
        "    # 2. calculate gradient respect to x                                          #\n",
        "    ###############################################################################\n",
        "    # *****BEGIN YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    delta_act =\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return delta_act\n",
        "\n",
        "\n",
        "def cross_entropy(pred, label):\n",
        "    \"\"\"\n",
        "    Calculate the cross entropy loss, L(pred, label)\n",
        "    This is for one image only\n",
        "    :param pred: predicted tensor\n",
        "    :param label: one-hot encoded label tensor\n",
        "    :return: the cross entropy loss, L(pred, label)\n",
        "    \"\"\"\n",
        "    ###############################################################################\n",
        "    # TODO:                                                                       #\n",
        "    # 1. convert pred into a probability distribution use softmax()               #\n",
        "    # 2. calculate cross entropy loss                                             #\n",
        "    ###############################################################################\n",
        "    # *****BEGIN YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    loss =\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cross_entropy_grad(pred, label):\n",
        "    \"\"\"\n",
        "    Calculate the gradient of cross entropy respect to pred\n",
        "    This is for one image only\n",
        "    :param pred: predicted tensor\n",
        "    :param label: one-hot encoded label tensor\n",
        "    :return: gradient of cross entropy respect to pred\n",
        "    \"\"\"\n",
        "\n",
        "    ###############################################################################\n",
        "    # TODO:                                                                       #\n",
        "    # 1. calculate element-wise gradient respect to pred = softmax(pred) - label  #\n",
        "    ###############################################################################\n",
        "    # *****BEGIN YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    delta_loss =\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return delta_loss\n",
        "\n",
        "\n",
        "def forward(w1, b1, w2, b2, x):\n",
        "    \"\"\"\n",
        "    forward operation\n",
        "    1. one linear layer followed by activation\n",
        "    2. one linear layer followed by activation\n",
        "    :param w1:\n",
        "    :param b1:\n",
        "    :param w2:\n",
        "    :param b2:\n",
        "    :param x: input tensor\n",
        "    :return: x0, s1, x1, s2, x2\n",
        "    \"\"\"\n",
        "    x0 = x\n",
        "    ###############################################################################\n",
        "    # TODO:                                                                       #\n",
        "    # 1. calculate s1 using w1, x0, b1                                            #\n",
        "    # 2. calculate x1 using activation()                                          #\n",
        "    # 3. calculate s2 using w2, x1, b2                                            #\n",
        "    # 4. calculate x2 using activation()                                          #\n",
        "    ###############################################################################\n",
        "    # *****BEGIN YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    s1 =\n",
        "    x1 =\n",
        "    s2 =\n",
        "    x2 =\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return x0, s1, x1, s2, x2\n",
        "\n",
        "\n",
        "def backward(w1, b1, w2, b2, t, x, s1, x1, s2, x2,\n",
        "             grad_dw1, grad_db1, grad_dw2, grad_db2):\n",
        "    \"\"\"\n",
        "    backward propagation, calculate dl_dw1, dl_db1, dl_dw2, dl_db2 using chain rule\n",
        "    :param w1:\n",
        "    :param b1:\n",
        "    :param w2:\n",
        "    :param b2:\n",
        "    :param t: label\n",
        "    :param x: input tensor\n",
        "    :param s1:\n",
        "    :param x1:\n",
        "    :param s2:\n",
        "    :param x2:\n",
        "    :param grad_dw1: gradient of w1\n",
        "    :param grad_db1: gradient of b1\n",
        "    :param grad_dw2: gradient of w2\n",
        "    :param grad_db2: gradient of b2\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    x0 = x\n",
        "    ###############################################################################\n",
        "    # TODO:                                                                       #\n",
        "    # 1. calculate grad_dx2 using x2, t                                             #\n",
        "    # 2. calculate grad_ds2 using s2, grad_dx2                                        #\n",
        "    # 3. calculate grad_dx1 using w2, grad_ds2                                        #\n",
        "    # 4. calculate grad_ds1 using s1, grad_dx1                                        #\n",
        "    # 5. calculate and accumulate grad_dw2 using grad_ds2, x1                         #\n",
        "    # 6. calculate and accumulate grad_db2 using grad_ds2                             #\n",
        "    # 7. calculate and accumulate grad_dw1 using grad_ds1, x0                         #\n",
        "    # 8. calculate and accumulate grad_db1 using grad_ds1                             #\n",
        "    ###############################################################################\n",
        "    # *****BEGIN YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    grad_dx2 =\n",
        "    grad_ds2 =\n",
        "    grad_dx1 =\n",
        "    grad_ds1 =\n",
        "\n",
        "    grad_dw2.add_()\n",
        "    grad_db2.add_()\n",
        "    grad_dw1.add_()\n",
        "    grad_db1.add_()\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
      ],
      "metadata": {
        "id": "7mow_daeaJpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Training loop (**12 points**)"
      ],
      "metadata": {
        "id": "ogiDUq5Kl4De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop, we have 10 classes\n",
        "nb_classes = 10\n",
        "nb_train_samples = len(train_loader)\n",
        "\n",
        "# set number of hidden neurons for first linear layer\n",
        "nb_hidden = 50\n",
        "# set learn rate and weights initialization std\n",
        "lr = 1e-1 / nb_train_samples\n",
        "init_std = 1e-6\n",
        "\n",
        "# initialize weights and biases to small values from normal distribution\n",
        "w1 = torch.empty(nb_hidden, image_size).normal_(0, init_std)\n",
        "b1 = torch.empty(nb_hidden).normal_(0, init_std)\n",
        "w2 = torch.empty(nb_classes, nb_hidden).normal_(0, init_std)\n",
        "b2 = torch.empty(nb_classes).normal_(0, init_std)\n",
        "\n",
        "# initialize empty tensor for gradients of weights and biases\n",
        "grad_dw1 = torch.empty(w1.size())\n",
        "grad_db1 = torch.empty(b1.size())\n",
        "grad_dw2 = torch.empty(w2.size())\n",
        "grad_db2 = torch.empty(b2.size())\n",
        "\n",
        "# run for 1000 epochs\n",
        "for k in range(1000):\n",
        "\n",
        "    # initialize loss and train error counts\n",
        "    acc_loss = 0\n",
        "    nb_train_errors = 0\n",
        "\n",
        "    ###############################################################################\n",
        "    # TODO:                                                                       #\n",
        "    # 1. clear all gradients of weights and biases using zero_()                  #\n",
        "    ###############################################################################\n",
        "    # *****BEGIN YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        train_target_one_hot = nn.functional.one_hot(y.squeeze(dim=0), num_classes=nb_classes)\n",
        "        ###############################################################################\n",
        "        # TODO:                                                                       #\n",
        "        # 1. do forward propagation use forward()                                     #\n",
        "        # 2. get prediction of x                                                      #\n",
        "        # 3. accumulate train error: nb_train_errors                                  #\n",
        "        # 4. accumulate train loss: acc_loss use cross_entropy                        #\n",
        "        # 5. do backward propagation use backward()                                   #\n",
        "        ###############################################################################\n",
        "        # *****BEGIN YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # Gradient step\n",
        "    ###############################################################################\n",
        "    # TODO:                                                                       #\n",
        "    # 1. step w1 using lr, dl_dw1                                                 #\n",
        "    # 2. step b1 using lr, dl_db1                                                 #\n",
        "    # 3. step w2 using lr, dl_dw2                                                 #\n",
        "    # 4. step b2 using lr, dl_db2                                                 #\n",
        "    ###############################################################################\n",
        "    # *****BEGIN YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    w1 =\n",
        "    b1 =\n",
        "    w2 =\n",
        "    b2 =\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # Val error, initialize val error count\n",
        "    nb_val_errors = 0\n",
        "\n",
        "    for x_val, y_val in val_loader:\n",
        "        ###############################################################################\n",
        "        # TODO:                                                                       #\n",
        "        # 1. do forward propagation use forward()                                     #\n",
        "        # 2. get prediction of x_val                                                  #\n",
        "        # 3. accumulate val error: nb_val_errors                                      #\n",
        "        ###############################################################################\n",
        "        # *****BEGIN YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # print train and val information at end of each epoch\n",
        "    print('{:d}: acc_train_loss {:.02f}, acc_train_accuracy {:.02f}%, val_accuracy {:.02f}%'\n",
        "          .format(k,\n",
        "                  acc_loss,\n",
        "                  100 - (100 * nb_train_errors) / len(train_loader),\n",
        "                  100 - (100 * nb_val_errors) / len(val_loader)))\n"
      ],
      "metadata": {
        "id": "5knm0wl63s0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Test model (**2 points**)"
      ],
      "metadata": {
        "id": "ptMUvPj7mWJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test accuracy\n",
        "nb_test_errors = 0\n",
        "for x_test, y_test in test_loader:\n",
        "    ###############################################################################\n",
        "    # TODO:                                                                       #\n",
        "    # 1. do forward propagation use forward()                                     #\n",
        "    # 2. get prediction of x_test                                                 #\n",
        "    # 3. accumulate val error: nb_test_errors                                     #\n",
        "    ###############################################################################\n",
        "    # *****BEGIN YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# print test set error\n",
        "print('test_accuracy {:.02f}%'.format(100 - (100 * nb_test_errors) / len(test_loader)))"
      ],
      "metadata": {
        "id": "-vTW3OEkjVn8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}